{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fb951b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b41cd6d",
   "metadata": {},
   "source": [
    "Output = weight $\\cdot$ input + bias is not unlike the equation for a line y = mx+b\n",
    "\n",
    "Adjusting the weight will impact the slope of the function.\n",
    "\n",
    "As we increase the bias, the function output overall shifts upward. \n",
    "\n",
    "If we decrease the bias, then the overall function output will move downward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6921ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3095c3f5",
   "metadata": {},
   "source": [
    "For a step function, if the neuron’s output value, which is calculated by sum(inputs · weights) + bias, is greater than 0, the neuron fires (so it would output a 1). Otherwise, it does not fire and would pass along a 0. The formula for a single neuron might look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8688a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sum(inputs * weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dad03",
   "metadata": {},
   "source": [
    "We then usually apply an activation function to this output, noted by activation():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a56f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = activation(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4178a4",
   "metadata": {},
   "source": [
    "### Coding a neuron\n",
    "\n",
    "Let’s say we have a single neuron with three inputs. \n",
    "\n",
    "Weights are initialized randomly. Biases set as zero \n",
    "\n",
    "Each input also needs a weight associated with it. \n",
    "\n",
    "There’s just one bias value per neuron.\n",
    " \n",
    "The values for weights and biases are what get 'trained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a00cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs  = [1.0, 2.0, 3.0] \n",
    "weights = [0.2, 0.8, -0.5] \n",
    "bias = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7fdcf",
   "metadata": {},
   "source": [
    "The neuron sums each input multiplied by that input’s weight, then adds the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31deb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = (inputs[0]* weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55f346",
   "metadata": {},
   "source": [
    "### Layer of neurons\n",
    "\n",
    "Neural networks typically have layers that consist of more than one neuron. \n",
    "\n",
    "Each neuron in a layer takes exactly the same input — the input given to the layer \n",
    "\n",
    "But contains its own set of weights and its own bias, producing a unique output. \n",
    "\n",
    "The layer’s output is a set of each of these outputs — one per each neuron. \n",
    "\n",
    "Let’s say we have a scenario with 3 neurons in a layer and 4 inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e5a543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.8, 1.21, 2.385]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [1, 2, 3, 2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1] \n",
    "weights2 = [0.5, -0.91, 0.26, -0.5] \n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
    "\n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "\n",
    "outputs = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias1,\n",
    "           inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias2,\n",
    "           inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] + bias3]\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff4e95",
   "metadata": {},
   "source": [
    "### Coding a neuron with NumPy\n",
    "\n",
    "Obviously, this way of coding won't scale\n",
    "\n",
    "We'll use numpy from now on...check out the Linear Algebra page \n",
    "\n",
    "Using the dot product and the addition of the vectors with NumPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d467e03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs  = [1.0, 2.0, 3.0, 2.5] \n",
    "weights = [0.2, 0.8, -0.5, 1.0] \n",
    "bias    = 2.0\n",
    "\n",
    "output  = np.dot(weights, inputs) + bias\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2400f",
   "metadata": {},
   "source": [
    "The layer bit doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fc8f2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs  = [1.0, 2.0, 3.0, 2.5]\n",
    "# weights = [[0.2, 0.80, -0.50, 1.00], [0.50 -0.91, 0.26, -0.50], [-0.26, -0.27, 0.17, 0.87]]\n",
    "# biases  = [2.0, 3.0, 0.5]\n",
    "# output  = np.dot(np.array(weights), inputs) + bias\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fc4d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4f61c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs  = [[1.0, 2.0, 3.0, 2.5], [2.0, 5.0, -1.0, 2.0],[-1.5, 2.7, 3.3, -0.8]] \n",
    "weights = [[0.2, 0.8, -0.5, 1.0],[0.5, -0.91, 0.26, -0.5],[-0.26, -0.27, 0.17, 0.87]] \n",
    "biases  = [2.0, 3.0, 0.5]\n",
    "\n",
    "outputs = np.dot(inputs, np.array(weights).T) + biases \n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805832bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedb7aac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
